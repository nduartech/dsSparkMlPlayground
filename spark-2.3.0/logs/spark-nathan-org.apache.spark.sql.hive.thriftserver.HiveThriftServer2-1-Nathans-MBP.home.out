Spark Command: /Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/bin/java -cp /Users/nathan/Repos/dsSparkMlPlayground/spark-2.3.0/conf/:/Users/nathan/Repos/dsSparkMlPlayground/spark-2.3.0/jars/* -Xmx1g org.apache.spark.deploy.SparkSubmit --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 --name Thrift JDBC/ODBC Server spark-internal
========================================
2020-01-17 00:52:04 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-01-17 00:52:06 INFO  HiveThriftServer2:2608 - Started daemon with process name: 10566@Nathans-MBP.home
2020-01-17 00:52:06 INFO  SignalUtils:54 - Registered signal handler for TERM
2020-01-17 00:52:06 INFO  SignalUtils:54 - Registered signal handler for HUP
2020-01-17 00:52:06 INFO  SignalUtils:54 - Registered signal handler for INT
2020-01-17 00:52:06 INFO  HiveThriftServer2:54 - Starting SparkContext
2020-01-17 00:52:07 INFO  SparkContext:54 - Running Spark version 2.3.0
2020-01-17 00:52:07 INFO  SparkContext:54 - Submitted application: Thrift JDBC/ODBC Server
2020-01-17 00:52:08 INFO  SecurityManager:54 - Changing view acls to: nathan
2020-01-17 00:52:08 INFO  SecurityManager:54 - Changing modify acls to: nathan
2020-01-17 00:52:08 INFO  SecurityManager:54 - Changing view acls groups to: 
2020-01-17 00:52:08 INFO  SecurityManager:54 - Changing modify acls groups to: 
2020-01-17 00:52:08 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(nathan); groups with view permissions: Set(); users  with modify permissions: Set(nathan); groups with modify permissions: Set()
2020-01-17 00:52:09 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 59162.
2020-01-17 00:52:10 INFO  SparkEnv:54 - Registering MapOutputTracker
2020-01-17 00:52:10 INFO  SparkEnv:54 - Registering BlockManagerMaster
2020-01-17 00:52:10 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-01-17 00:52:10 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2020-01-17 00:52:10 INFO  DiskBlockManager:54 - Created local directory at /private/var/folders/mg/4h87x2h168s28_vfyf9_rjyw0000gn/T/blockmgr-d6ae861a-7f6f-42fa-ab3d-2262fb1ebcf3
2020-01-17 00:52:10 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB
2020-01-17 00:52:10 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2020-01-17 00:52:11 INFO  log:192 - Logging initialized @273115ms
2020-01-17 00:52:12 INFO  Server:346 - jetty-9.3.z-SNAPSHOT
2020-01-17 00:52:12 INFO  Server:414 - Started @273872ms
2020-01-17 00:52:12 INFO  AbstractConnector:278 - Started ServerConnector@25985559{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-01-17 00:52:12 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.
2020-01-17 00:52:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4c432866{/jobs,null,AVAILABLE,@Spark}
2020-01-17 00:52:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@61019f59{/jobs/json,null,AVAILABLE,@Spark}
2020-01-17 00:52:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@62e8f862{/jobs/job,null,AVAILABLE,@Spark}
2020-01-17 00:52:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3c49fab6{/jobs/job/json,null,AVAILABLE,@Spark}
2020-01-17 00:52:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@515f4131{/stages,null,AVAILABLE,@Spark}
2020-01-17 00:52:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@74518890{/stages/json,null,AVAILABLE,@Spark}
2020-01-17 00:52:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4c5204af{/stages/stage,null,AVAILABLE,@Spark}
2020-01-17 00:52:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6c2d4cc6{/stages/stage/json,null,AVAILABLE,@Spark}
2020-01-17 00:52:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@30865a90{/stages/pool,null,AVAILABLE,@Spark}
2020-01-17 00:52:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6134ac4a{/stages/pool/json,null,AVAILABLE,@Spark}
2020-01-17 00:52:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@777c9dc9{/storage,null,AVAILABLE,@Spark}
2020-01-17 00:52:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@71b1a49c{/storage/json,null,AVAILABLE,@Spark}
2020-01-17 00:52:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@73e132e0{/storage/rdd,null,AVAILABLE,@Spark}
2020-01-17 00:52:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3773862a{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-01-17 00:52:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2472c7d8{/environment,null,AVAILABLE,@Spark}
2020-01-17 00:52:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@589b028e{/environment/json,null,AVAILABLE,@Spark}
2020-01-17 00:52:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@22175d4f{/executors,null,AVAILABLE,@Spark}
2020-01-17 00:52:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@9fecdf1{/executors/json,null,AVAILABLE,@Spark}
2020-01-17 00:52:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3b809711{/executors/threadDump,null,AVAILABLE,@Spark}
2020-01-17 00:52:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3b0f7d9d{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-01-17 00:52:13 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@236ab296{/static,null,AVAILABLE,@Spark}
2020-01-17 00:52:13 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6594402a{/,null,AVAILABLE,@Spark}
2020-01-17 00:52:13 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@30f4b1a6{/api,null,AVAILABLE,@Spark}
2020-01-17 00:52:13 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6c2f1700{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-01-17 00:52:13 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@350b3a17{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-01-17 00:52:13 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://nathans-mbp.home:4040
2020-01-17 00:52:14 INFO  Executor:54 - Starting executor ID driver on host localhost
2020-01-17 00:52:14 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59166.
2020-01-17 00:52:14 INFO  NettyBlockTransferService:54 - Server created on nathans-mbp.home:59166
2020-01-17 00:52:14 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-01-17 00:52:14 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, nathans-mbp.home, 59166, None)
2020-01-17 00:52:14 INFO  BlockManagerMasterEndpoint:54 - Registering block manager nathans-mbp.home:59166 with 366.3 MB RAM, BlockManagerId(driver, nathans-mbp.home, 59166, None)
2020-01-17 00:52:14 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, nathans-mbp.home, 59166, None)
2020-01-17 00:52:14 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, nathans-mbp.home, 59166, None)
2020-01-17 00:52:16 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@79a1728c{/metrics/json,null,AVAILABLE,@Spark}
2020-01-17 00:52:16 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/nathan/Repos/dsSparkMlPlayground/spark-warehouse/').
2020-01-17 00:52:16 INFO  SharedState:54 - Warehouse path is 'file:/Users/nathan/Repos/dsSparkMlPlayground/spark-warehouse/'.
2020-01-17 00:52:16 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@55b8dbda{/SQL,null,AVAILABLE,@Spark}
2020-01-17 00:52:16 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3b569985{/SQL/json,null,AVAILABLE,@Spark}
2020-01-17 00:52:16 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@31ddd4a4{/SQL/execution,null,AVAILABLE,@Spark}
2020-01-17 00:52:16 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1a5f7e7c{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-01-17 00:52:16 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@529cfee5{/static/sql,null,AVAILABLE,@Spark}
2020-01-17 00:52:18 INFO  HiveUtils:54 - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
2020-01-17 01:52:34 INFO  HiveMetaStore:589 - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2020-01-17 01:52:34 INFO  ObjectStore:289 - ObjectStore, initialize called
2020-01-17 01:52:34 INFO  Persistence:77 - Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2020-01-17 01:52:34 INFO  Persistence:77 - Property datanucleus.cache.level2 unknown - will be ignored
2020-01-17 01:53:59 INFO  ObjectStore:370 - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2020-01-17 01:54:07 INFO  Datastore:77 - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2020-01-17 01:54:07 INFO  Datastore:77 - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2020-01-17 01:54:09 INFO  Datastore:77 - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2020-01-17 01:54:09 INFO  Datastore:77 - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2020-01-17 01:54:10 INFO  MetaStoreDirectSql:139 - Using direct SQL, underlying DB is DERBY
2020-01-17 01:54:10 INFO  ObjectStore:272 - Initialized ObjectStore
2020-01-17 01:54:10 WARN  ObjectStore:6666 - Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
2020-01-17 01:54:11 WARN  ObjectStore:568 - Failed to get database default, returning NoSuchObjectException
2020-01-17 01:54:11 INFO  HiveMetaStore:663 - Added admin role in metastore
2020-01-17 01:54:11 INFO  HiveMetaStore:672 - Added public role in metastore
2020-01-17 01:54:12 INFO  HiveMetaStore:712 - No user is added in admin role, since config is empty
2020-01-17 01:54:12 INFO  HiveMetaStore:746 - 0: get_all_databases
2020-01-17 01:54:12 INFO  audit:371 - ugi=nathan	ip=unknown-ip-addr	cmd=get_all_databases	
2020-01-17 01:54:12 INFO  HiveMetaStore:746 - 0: get_functions: db=default pat=*
2020-01-17 01:54:12 INFO  audit:371 - ugi=nathan	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
2020-01-17 01:54:12 INFO  Datastore:77 - The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
2020-01-17 01:54:13 INFO  SessionState:641 - Created local directory: /var/folders/mg/4h87x2h168s28_vfyf9_rjyw0000gn/T/66273729-1cd1-4018-a410-c688fdc753c1_resources
2020-01-17 01:54:13 INFO  SessionState:641 - Created HDFS directory: /tmp/hive/nathan/66273729-1cd1-4018-a410-c688fdc753c1
2020-01-17 01:54:13 INFO  SessionState:641 - Created local directory: /var/folders/mg/4h87x2h168s28_vfyf9_rjyw0000gn/T/nathan/66273729-1cd1-4018-a410-c688fdc753c1
2020-01-17 01:54:13 INFO  SessionState:641 - Created HDFS directory: /tmp/hive/nathan/66273729-1cd1-4018-a410-c688fdc753c1/_tmp_space.db
2020-01-17 01:54:13 INFO  HiveClientImpl:54 - Warehouse location for Hive client (version 1.2.2) is file:/Users/nathan/Repos/dsSparkMlPlayground/spark-warehouse/
2020-01-17 01:54:13 INFO  HiveMetaStore:746 - 0: get_database: default
2020-01-17 01:54:13 INFO  audit:371 - ugi=nathan	ip=unknown-ip-addr	cmd=get_database: default	
2020-01-17 01:54:14 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint
2020-01-17 01:54:14 INFO  HiveUtils:54 - Initializing execution hive, version 1.2.1
2020-01-17 01:54:15 INFO  HiveMetaStore:589 - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2020-01-17 01:54:15 INFO  ObjectStore:289 - ObjectStore, initialize called
2020-01-17 01:54:16 INFO  Persistence:77 - Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2020-01-17 01:54:16 INFO  Persistence:77 - Property datanucleus.cache.level2 unknown - will be ignored
2020-01-17 01:54:21 INFO  ObjectStore:370 - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2020-01-17 01:54:24 INFO  Datastore:77 - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2020-01-17 01:54:24 INFO  Datastore:77 - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2020-01-17 01:54:27 INFO  Datastore:77 - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2020-01-17 01:54:27 INFO  Datastore:77 - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2020-01-17 01:54:28 INFO  MetaStoreDirectSql:139 - Using direct SQL, underlying DB is DERBY
2020-01-17 01:54:28 INFO  ObjectStore:272 - Initialized ObjectStore
2020-01-17 01:54:28 WARN  ObjectStore:6666 - Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
2020-01-17 01:54:29 WARN  ObjectStore:568 - Failed to get database default, returning NoSuchObjectException
2020-01-17 01:54:29 INFO  HiveMetaStore:663 - Added admin role in metastore
2020-01-17 01:54:29 INFO  HiveMetaStore:672 - Added public role in metastore
2020-01-17 01:54:29 INFO  HiveMetaStore:712 - No user is added in admin role, since config is empty
2020-01-17 01:54:30 INFO  HiveMetaStore:746 - 0: get_all_databases
2020-01-17 01:54:30 INFO  audit:371 - ugi=nathan	ip=unknown-ip-addr	cmd=get_all_databases	
2020-01-17 01:54:30 INFO  HiveMetaStore:746 - 0: get_functions: db=default pat=*
2020-01-17 01:54:30 INFO  audit:371 - ugi=nathan	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
2020-01-17 01:54:30 INFO  Datastore:77 - The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
2020-01-17 01:54:30 INFO  SessionState:641 - Created local directory: /var/folders/mg/4h87x2h168s28_vfyf9_rjyw0000gn/T/fdad932b-d907-4292-8e11-53e8c40e6343_resources
2020-01-17 01:54:30 INFO  SessionState:641 - Created HDFS directory: /tmp/hive/nathan/fdad932b-d907-4292-8e11-53e8c40e6343
2020-01-17 01:54:31 INFO  SessionState:641 - Created local directory: /var/folders/mg/4h87x2h168s28_vfyf9_rjyw0000gn/T/nathan/fdad932b-d907-4292-8e11-53e8c40e6343
2020-01-17 01:54:31 INFO  SessionState:641 - Created HDFS directory: /tmp/hive/nathan/fdad932b-d907-4292-8e11-53e8c40e6343/_tmp_space.db
2020-01-17 01:54:31 INFO  HiveClientImpl:54 - Warehouse location for Hive client (version 1.2.2) is file:/Users/nathan/Repos/dsSparkMlPlayground/spark-warehouse/
2020-01-17 01:54:31 INFO  SessionManager:133 - Operation log root directory is created: /var/folders/mg/4h87x2h168s28_vfyf9_rjyw0000gn/T/nathan/operation_logs
2020-01-17 01:54:31 INFO  SessionManager:88 - HiveServer2: Background operation thread pool size: 100
2020-01-17 01:54:31 INFO  SessionManager:90 - HiveServer2: Background operation thread wait queue size: 100
2020-01-17 01:54:31 INFO  SessionManager:93 - HiveServer2: Background operation thread keepalive time: 10 seconds
2020-01-17 01:54:31 INFO  AbstractService:89 - Service:OperationManager is inited.
2020-01-17 01:54:31 INFO  AbstractService:89 - Service:SessionManager is inited.
2020-01-17 01:54:31 INFO  AbstractService:101 - Service: CLIService is inited.
2020-01-17 01:54:31 INFO  AbstractService:89 - Service:ThriftBinaryCLIService is inited.
2020-01-17 01:54:31 INFO  AbstractService:101 - Service: HiveServer2 is inited.
2020-01-17 01:54:31 INFO  AbstractService:104 - Service:OperationManager is started.
2020-01-17 01:54:31 INFO  AbstractService:104 - Service:SessionManager is started.
2020-01-17 01:54:31 INFO  AbstractService:104 - Service:CLIService is started.
2020-01-17 01:54:31 INFO  ObjectStore:289 - ObjectStore, initialize called
2020-01-17 01:54:31 INFO  Query:77 - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
2020-01-17 01:54:31 INFO  MetaStoreDirectSql:139 - Using direct SQL, underlying DB is DERBY
2020-01-17 01:54:31 INFO  ObjectStore:272 - Initialized ObjectStore
2020-01-17 01:54:31 INFO  HiveMetaStore:746 - 0: get_databases: default
2020-01-17 01:54:31 INFO  audit:371 - ugi=nathan	ip=unknown-ip-addr	cmd=get_databases: default	
2020-01-17 01:54:31 INFO  HiveMetaStore:746 - 0: Shutting down the object store...
2020-01-17 01:54:31 INFO  audit:371 - ugi=nathan	ip=unknown-ip-addr	cmd=Shutting down the object store...	
2020-01-17 01:54:31 INFO  HiveMetaStore:746 - 0: Metastore shutdown complete.
2020-01-17 01:54:31 INFO  audit:371 - ugi=nathan	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
2020-01-17 01:54:31 INFO  AbstractService:104 - Service:ThriftBinaryCLIService is started.
2020-01-17 01:54:31 INFO  AbstractService:104 - Service:HiveServer2 is started.
2020-01-17 01:54:31 INFO  HiveThriftServer2:54 - HiveThriftServer2 started
2020-01-17 01:54:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@96c840a{/sqlserver,null,AVAILABLE,@Spark}
2020-01-17 01:54:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5c81ddab{/sqlserver/json,null,AVAILABLE,@Spark}
2020-01-17 01:54:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@394b9e22{/sqlserver/session,null,AVAILABLE,@Spark}
2020-01-17 01:54:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6e0f259e{/sqlserver/session/json,null,AVAILABLE,@Spark}
2020-01-17 01:54:31 INFO  ThriftCLIService:98 - Starting ThriftBinaryCLIService on port 10000 with 5...500 worker threads
